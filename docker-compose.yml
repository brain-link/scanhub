x-airflow-common:
  &airflow-common
  # In order to add custom dependencies or upgrade provider packages you can use your extended image.
  # Comment the image line, place your Dockerfile in the directory where you placed the docker-compose.yaml
  # and uncomment the "build" line below, Then run `docker-compose build` to build the images.
  # image: ${AIRFLOW_IMAGE_NAME:-apache/airflow:2.10.5-python3.10}
  build: ./services/airflow
  environment:
    &airflow-common-env
    AIRFLOW__CORE__EXECUTOR: LocalExecutor
    AIRFLOW__DATABASE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    # For backward compatibility, with Airflow <2.3
    AIRFLOW__CORE__SQL_ALCHEMY_CONN: postgresql+psycopg2://airflow:airflow@airflow-db/airflow
    AIRFLOW__CORE__FERNET_KEY: ''
    AIRFLOW__CORE__DAGS_ARE_PAUSED_AT_CREATION: 'true'
    AIRFLOW__CORE__LOAD_EXAMPLES: 'false'  # 'true' to see some tutorials and examples
    AIRFLOW__API__AUTH_BACKENDS: 'airflow.api.auth.backend.basic_auth'
    AIRFLOW__SECRETS__BACKEND: airflow.secrets.local_filesystem.LocalFilesystemBackend
    AIRFLOW__SECRETS__BACKEND_KWARGS: '{"variables_file_path": "/opt/secrets/variables.yaml", "connections_file_path": "/opt/secrets/connections.yaml"}'
    AIRFLOW__WEBSERVER__EXPOSE_CONFIG: 'true'
    AIRFLOW__CLI__ENDPOINT_URL: http://localhost:8081
  volumes:
    - ./services/airflow/dags:/opt/airflow/dags
    - ./services/airflow/logs:/opt/airflow/logs
    - ./services/airflow/plugins:/opt/airflow/plugins
    # - ${AIRFLOW_DAGS}:/opt/airflow/dags
    # - ${AIRFLOW_LOGS}:/opt/airflow/logs
    # - ${AIRFLOW_PLUGINS}:/opt/airflow/plugins
    - ./services/airflow/secrets:/opt/secrets
    - data_lake:/opt/airflow/data_lake
  user: "${AIRFLOW_UID:-50000}:0"
  depends_on:
    &airflow-common-depends-on
    airflow-database:
      condition: service_healthy

services:

  # SERVICES
  # Workflow manager, device manager and exam manager create data tables in postgres DB
  workflow-manager:
    container_name: workflow-manager
    build: 
      context: ./services/workflow-manager
      args: 
        - BASE_IMG=${SCANHUB_BASE_IMAGE}
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8000
    volumes:
      - ./services/workflow-manager/app/:/app/
      - data_lake:/app/data_lake/
    ports:
      - 8001:8000
    environment:
      - DB_URI=postgresql://brainlink:9ArU5*Sr@scanhub-database/sh_db
      - DB_URI_ASYNC=postgresql+asyncpg://brainlink:9ArU5*Sr@scanhub-database/sh_db
      - ORCHESTRATION_ENGINE=AIRFLOW
      - KESTRA_API_URL=http://kestra:8080
      - AIRFLOW_API_URL=http://airflow-webserver:8080
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://workflow-manager:8000/api/v1/workflowmanager/health/readiness"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      scanhub-database:
        condition: service_healthy

  device-manager:
    container_name: device-manager
    build:
      context: ./services/device-manager
      args: 
        - BASE_IMG=${SCANHUB_BASE_IMAGE}
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8000
    volumes:
      - ./services/device-manager/app/:/app/
      - data_lake:/app/data_lake/
    ports:
      - 8002:8000
    environment:
      - DB_URI=postgresql://brainlink:9ArU5*Sr@scanhub-database/sh_db
      - DB_URI_ASYNC=postgresql+asyncpg://brainlink:9ArU5*Sr@scanhub-database/sh_db
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://device-manager:8000/api/v1/device/health/readiness"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      scanhub-database:
        condition: service_healthy

  exam-manager:
    container_name: exam-manager
    build: 
      context: ./services/exam-manager
      args: 
        - BASE_IMG=${SCANHUB_BASE_IMAGE}
        # - BASE_IMG=scanhub-base
    command: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
    volumes:
      - ./services/exam-manager/app/:/app/
      - data_lake:/app/data_lake/
    ports:
      - 8004:8000
    environment:
      - DB_URI=postgresql://brainlink:9ArU5*Sr@scanhub-database/sh_db
      - DB_URI_ASYNC=postgresql+asyncpg://brainlink:9ArU5*Sr@scanhub-database/sh_db
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://exam-manager:8000/api/v1/exam/health/readiness"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      scanhub-database:
        condition: service_healthy
      device-manager:
        condition: service_healthy
      workflow-manager:
        condition: service_healthy

  patient-manager:
    container_name: patient-manager
    build: 
      context: ./services/patient-manager
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8100
    volumes:
    - ./services/patient-manager/app:/app/
    ports:
      - 8100:8100
    environment:
      - DB_URI=postgresql://brainlink:data@patient-database/patients-data
      - DB_URI_ASYNC=postgresql+asyncpg://brainlink:data@patient-database/patients-data
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://patient-manager:8100/api/v1/patient/health/readiness"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      patient-database:
        condition: service_healthy

  user-login-manager:
    container_name: user-login-manager
    build: 
      context: ./services/user-login-manager
      args: 
        - BASE_IMG=${SCANHUB_BASE_IMAGE}
        # - BASE_IMG=scanhub-base
    command: uvicorn app.main:app --reload --host 0.0.0.0 --port 8000
    volumes:
      - ./services/user-login-manager/app/:/app/
      - data_lake:/app/data_lake/
    ports:
      - 8005:8000
    environment:
      - DB_URI=postgresql://brainlink:9ArU5*Sr@scanhub-database/sh_db
      - DB_URI_ASYNC=postgresql+asyncpg://brainlink:9ArU5*Sr@scanhub-database/sh_db
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://user-login-manager:8000/api/v1/userlogin/health/readiness"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      scanhub-database:
        condition: service_healthy


  # MRI Sequence Service
  sequence-manager:
    container_name: sequence-manager
    build: 
      context: ./services/mri/sequence-manager
      args: 
        - BASE_IMG=${SCANHUB_BASE_IMAGE}
    command: uvicorn main:app --reload --host 0.0.0.0 --port 8000
    volumes:
      - ./services/mri/sequence-manager/app/:/app/
      - data_lake:/app/data_lake/
    ports:
      - 8003:8000
    environment:
      - MONGODB_USER=root
      - MONGODB_PASSWORD=example
      - MONGODB_HOST=sequence-database
      - MONGODB_PORT=27017
      - MONGODB_DB=mri_sequences_db
      - MONGODB_COLLECTION_NAME=mri_sequences
    healthcheck:
      # TODO: host.docker.internal is only working with docker desktop, must be replaced in product
      test: ["CMD", "curl", "http://sequence-manager:8000/api/v1/mri/sequences/health"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      sequence-database:
        condition: service_healthy


  # Airflow services
  airflow-webserver:
    <<: *airflow-common
    command: webserver
    ports:
      - 8081:8080
    healthcheck:
      test: [ "CMD", "curl", "--fail", "http://localhost:8081/health" ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-scheduler:
    <<: *airflow-common
    command: scheduler
    healthcheck:
      test: [ "CMD-SHELL", 'airflow jobs check --job-type SchedulerJob --hostname "$${HOSTNAME}"' ]
      interval: 10s
      timeout: 10s
      retries: 5
    restart: always
    depends_on:
      <<: *airflow-common-depends-on
      airflow-init:
        condition: service_completed_successfully

  airflow-init:
    <<: *airflow-common
    entrypoint: /bin/bash
    # yamllint disable rule:line-length
    command:
      - -c
      - |
        function ver() {
          printf "%04d%04d%04d%04d" $${1//./ }
        }
        if [[ -z "${AIRFLOW_UID}" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: AIRFLOW_UID not set!\e[0m"
          echo "If you are on Linux, you SHOULD follow the instructions below to set "
          echo "AIRFLOW_UID environment variable, otherwise files will be owned by root."
          echo "For other operating systems you can get rid of the warning with manually created .env file:"
          echo "    See: https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#setting-the-right-airflow-user"
          echo
        fi
        one_meg=1048576
        mem_available=$$(($$(getconf _PHYS_PAGES) * $$(getconf PAGE_SIZE) / one_meg))
        cpus_available=$$(grep -cE 'cpu[0-9]+' /proc/stat)
        disk_available=$$(df / | tail -1 | awk '{print $$4}')
        warning_resources="false"
        if (( mem_available < 4000 )) ; then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough memory available for Docker.\e[0m"
          echo "At least 4GB of memory required. You have $$(numfmt --to iec $$((mem_available * one_meg)))"
          echo
          warning_resources="true"
        fi
        if (( cpus_available < 2 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough CPUS available for Docker.\e[0m"
          echo "At least 2 CPUs recommended. You have $${cpus_available}"
          echo
          warning_resources="true"
        fi
        if (( disk_available < one_meg * 10 )); then
          echo
          echo -e "\033[1;33mWARNING!!!: Not enough Disk space available for Docker.\e[0m"
          echo "At least 10 GBs recommended. You have $$(numfmt --to iec $$((disk_available * 1024 )))"
          echo
          warning_resources="true"
        fi
        if [[ $${warning_resources} == "true" ]]; then
          echo
          echo -e "\033[1;33mWARNING!!!: You have not enough resources to run Airflow (see above)!\e[0m"
          echo "Please follow the instructions to increase amount of resources available:"
          echo "   https://airflow.apache.org/docs/apache-airflow/stable/start/docker.html#before-you-begin"
          echo
        fi
        mkdir -p /sources/logs /sources/dags /sources/plugins
        chown -R "${AIRFLOW_UID}:0" /sources/{logs,dags,plugins}
        exec /entrypoint airflow version
    # yamllint enable rule:line-length
    environment:
      <<: *airflow-common-env
      _AIRFLOW_DB_UPGRADE: 'true'
      _AIRFLOW_WWW_USER_CREATE: 'true'
      _AIRFLOW_WWW_USER_USERNAME: ${_AIRFLOW_WWW_USER_USERNAME:-airflow}
      _AIRFLOW_WWW_USER_PASSWORD: ${_AIRFLOW_WWW_USER_PASSWORD:-airflow}
      _PIP_ADDITIONAL_REQUIREMENTS: ''
    user: "0:0"
    volumes:
      - .:/sources

  # DATABASES
  # Airflow Postgres Database:
  airflow-database:
    container_name: airflow-db
    image: postgres:15.2-bullseye
    environment:
      POSTGRES_USER: airflow
      POSTGRES_PASSWORD: airflow
      POSTGRES_DB: airflow
    volumes:
      - airflow-db-volume:/var/lib/postgresql/data
    ports:
      - 5434:5432
    healthcheck:
      test: ["CMD", "pg_isready", "-U", "airflow"]
      interval: 5s
      retries: 5
    restart: always

  # Scanhub Postgres Database: Contains exams, procedures, records, jobs, devices and workflows 
  scanhub-database:
    container_name: db-scanhub
    image: postgres:15.2-bullseye
    restart: always # unless-stopped
    volumes:
      - postgres_data_scanhub:/var/lib/postgresql/data/
      - ./init_db.sql:/docker-entrypoint-initdb.d/init_db.sql
    environment:
      POSTGRES_USER: brainlink
      POSTGRES_PASSWORD: 9ArU5*Sr
      POSTGRES_DB: sh_db
    healthcheck:  
      # Workflow, device and exam microservices depend on healthy postgres database
      test: ["CMD-SHELL", "pg_isready"]
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 5s 
    ports:
      - "5433:5432"

  # Patient database
  patient-database:
    container_name: patient-db
    image: postgres:15.2-bullseye
    restart: always
    volumes:
      - postgres_data_patient:/var/lib/postgresql/data/
    environment:
      POSTGRES_USER: brainlink
      POSTGRES_DB: patients-data
      POSTGRES_PASSWORD: data
    ports:
      - "5432:5432"
    healthcheck:  
      # init-db container is started, after health check passed
      test: ["CMD-SHELL", "pg_isready", "-d", "db_prod"]
      interval: 10s
      timeout: 5s
      retries: 5
      start_period: 5s 


  # MRI Sequence mongo database: Contains (pulseq) mri sequence files
  sequence-database:
    container_name: db-sequence
    image: mongo
    restart: always # unless-stopped
    volumes:
      - mongodb_data_sequence:/data/db
    # environment:
    #   MONGO_INITDB_ROOT_USERNAME: root
    #   MONGO_INITDB_ROOT_PASSWORD: 9ArU5*Sr
    healthcheck:
      test: echo 'db.runCommand("ping").ok' | mongosh sequence-database:27017/test --quiet
      interval: 5s
      timeout: 5s
      retries: 10
      start_period: 20s
    ports:
      - "27017:27017"


  # API Gateway
  nginx:
    container_name: api-gateway
    image: nginx:latest
    ports:
      - "8080:8080"
    volumes:
      - ./infrastructure/nginx_config.conf:/etc/nginx/conf.d/default.conf
    healthcheck:
      test: ["CMD", "curl", "http://api-gateway:8080/health"]
      interval: 5s
      timeout: 5s
      retries: 10
    depends_on:
      device-manager:
        condition: service_healthy
      workflow-manager:
        condition: service_healthy
      exam-manager: 
        condition: service_healthy
      sequence-manager:
        condition: service_started
      patient-manager:
        condition: service_healthy
      user-login-manager:
        condition: service_healthy
    extra_hosts:                                # somehow needed to find host.docker.internal under linux
      - 'host.docker.internal:host-gateway'     # for the access to host.docker.internal to work the firewall of the host needs to allow incoming connections
  

  # FRONTEND/SCANHUB-UI
  scanhub-ui:
    profiles: ["with-frontend"]
    container_name: scanhub-ui
    build:
      context: ./scanhub-ui
      dockerfile: ./Dockerfile
      args:
        NODE_ENV: "development" # use "development" or "production"
    volumes:
      - ./scanhub-ui/src:/app/src
    depends_on:
      - nginx
    ports:
      - 3000:3000


volumes:
  data_lake:
  mongodb_data_sequence:
  postgres_data_scanhub:
  postgres_data_patient:
  airflow-db-volume: